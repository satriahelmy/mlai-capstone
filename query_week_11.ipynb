{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cca3f44-bfec-4e9f-913f-3dcb27c8d8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parsed 8 input vectors, 8 outputs.\n",
      "\n",
      "=== Function 1 (2D) ===\n",
      "  Data size: 20, Output range: [-0.0036, 64.0000]\n",
      "  Best predicted output: 15.7323\n",
      "  Cluster-based centroid output: 10.9567 (silhouette=0.620)\n",
      "  Query to submit: 0.140392-0.564707\n",
      "\n",
      "=== Function 2 (2D) ===\n",
      "  Data size: 20, Output range: [-0.0656, 3.1124]\n",
      "  Best predicted output: 0.7477\n",
      "  Cluster-based centroid output: 0.7155 (silhouette=0.582)\n",
      "  Query to submit: 0.748738-0.731142\n",
      "\n",
      "=== Function 3 (3D) ===\n",
      "  Data size: 25, Output range: [-0.3989, 71.0000]\n",
      "  Best predicted output: 24.0944\n",
      "  Cluster-based centroid output: 14.8215 (silhouette=0.401)\n",
      "  Query to submit: 0.255475-0.643362-0.830987\n",
      "\n",
      "=== Function 4 (4D) ===\n",
      "  Data size: 40, Output range: [-32.6257, 64.0000]\n",
      "  Best predicted output: 3.3990\n",
      "  Cluster-based centroid output: -3.1873 (silhouette=0.304)\n",
      "  Query to submit: 0.217797-0.347217-0.281739-0.367782\n",
      "\n",
      "=== Function 5 (4D) ===\n",
      "  Data size: 30, Output range: [0.1129, 4440.5227]\n",
      "  Best predicted output: 3215.9968\n",
      "  Cluster-based centroid output: 3052.9151 (silhouette=0.418)\n",
      "  Query to submit: 0.063400-0.950836-0.929442-0.941028\n",
      "\n",
      "=== Function 6 (5D) ===\n",
      "  Data size: 30, Output range: [-2.5712, 64.0000]\n",
      "  Best predicted output: 8.7094\n",
      "  Cluster-based centroid output: -1.3860 (silhouette=0.300)\n",
      "  Query to submit: 0.518778-0.260138-0.083378-0.682770-0.204990\n",
      "\n",
      "=== Function 7 (6D) ===\n",
      "  Data size: 40, Output range: [-0.0787, 2.0091]\n",
      "  Best predicted output: 1.7486\n",
      "  Cluster-based centroid output: 1.8713 (silhouette=0.248)\n",
      "  Query to submit: 0.050980-0.354120-0.261869-0.292216-0.315837-0.730668\n",
      "\n",
      "=== Function 8 (8D) ===\n",
      "  Data size: 50, Output range: [5.5922, 64.0000]\n",
      "  Best predicted output: 40.4237\n",
      "  Cluster-based centroid output: 21.7257 (silhouette=0.177)\n",
      "  Query to submit: 0.114570-0.524669-0.486985-0.504971-0.498225-0.559502-0.219654-0.904949\n",
      "\n",
      "ðŸ’¾ Saved all week11 queries to week11_queries.txt\n",
      "ðŸ“Š Cluster analysis logs saved under week11_logs/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, re, ast, datetime, json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "base_path = \"data/\"\n",
    "inputs_txt = \"week10/inputs.txt\"       # file hasil minggu sebelumnya\n",
    "outputs_txt = \"week10/outputs.txt\"\n",
    "n_candidates = 4000\n",
    "log_folder = \"week11_logs\"\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD INPUTS (ambil batch terakhir)\n",
    "# =========================\n",
    "def load_inputs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read().strip()\n",
    "    batches = re.split(r\"\\]\\s*\\n\\s*\\[\", text)\n",
    "    last_batch = \"[\" + batches[-1].strip().lstrip(\"[\").rstrip(\"]\") + \"]\"\n",
    "    last_batch = re.sub(r'array\\(', '', last_batch).replace(')', '')\n",
    "    chunks = re.findall(r'\\[([^\\[\\]]+)\\]', last_batch)\n",
    "    return [np.array([float(x) for x in ch.split(\",\") if x.strip()]) for ch in chunks]\n",
    "\n",
    "def load_outputs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read().strip()\n",
    "    batches = re.split(r\"\\]\\s*\\n\\s*\\[\", text)\n",
    "    last_batch = batches[-1].strip()\n",
    "    last_batch = last_batch.replace(\"np.float64(\", \"\").replace(\")\", \"\")\n",
    "    last_batch = last_batch.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    last_batch = re.sub(r\"[^\\deE\\-\\.\\,\\s]\", \"\", last_batch)\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", last_batch)\n",
    "    return np.array([float(x) for x in numbers], dtype=float)\n",
    "\n",
    "# =========================\n",
    "# LOAD NEW DATA\n",
    "# =========================\n",
    "new_inputs = load_inputs(inputs_txt)\n",
    "new_outputs = load_outputs(outputs_txt)\n",
    "print(f\"âœ… Parsed {len(new_inputs)} input vectors, {len(new_outputs)} outputs.\")\n",
    "\n",
    "# =========================\n",
    "# TRAIN SURROGATE & CLUSTER-AWARE QUERYING\n",
    "# =========================\n",
    "queries_out = []\n",
    "cluster_info = {}\n",
    "\n",
    "for i in range(1, 9):\n",
    "    folder = os.path.join(base_path, f\"function_{i}\")\n",
    "    X_prev = np.load(os.path.join(folder, \"week10_inputs.npy\"))\n",
    "    y_prev = np.load(os.path.join(folder, \"week10_outputs.npy\"))\n",
    "\n",
    "    # Gabungkan data lama + batch baru\n",
    "    X_combined = np.vstack([X_prev, new_inputs[i-1].reshape(1, -1)])\n",
    "    y_combined = np.append(y_prev, new_outputs[i-1])\n",
    "\n",
    "    np.save(os.path.join(folder, \"week11_inputs.npy\"), X_combined)\n",
    "    np.save(os.path.join(folder, \"week11_outputs.npy\"), y_combined)\n",
    "\n",
    "    dim = X_combined.shape[1]\n",
    "    print(f\"\\n=== Function {i} ({dim}D) ===\")\n",
    "    print(f\"  Data size: {len(X_combined)}, Output range: [{y_combined.min():.4f}, {y_combined.max():.4f}]\")\n",
    "\n",
    "    # Build surrogate\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        MLPRegressor(hidden_layer_sizes=(512, 256, 128),\n",
    "                     activation='relu',\n",
    "                     solver='adam',\n",
    "                     alpha=3e-4,\n",
    "                     learning_rate_init=0.001,\n",
    "                     max_iter=3000,\n",
    "                     random_state=42)\n",
    "    )\n",
    "    model.fit(X_combined, y_combined)\n",
    "\n",
    "    # Predict on candidate grid\n",
    "    candidates = np.random.uniform(0, 1, (n_candidates, dim))\n",
    "    preds = model.predict(candidates)\n",
    "\n",
    "    # --- CLUSTER ANALYSIS ---\n",
    "    k_opt = min(4, len(X_combined)//2)  # avoid too many clusters\n",
    "    kmeans = KMeans(n_clusters=k_opt, n_init=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_combined)\n",
    "    sil_score = silhouette_score(X_combined, labels) if len(np.unique(labels)) > 1 else 0\n",
    "    cluster_means = [np.mean(y_combined[labels == k]) for k in range(k_opt)]\n",
    "    best_cluster = np.argmax(cluster_means)\n",
    "    centroid = kmeans.cluster_centers_[best_cluster]\n",
    "    cluster_pred = model.predict(centroid.reshape(1, -1))[0]\n",
    "\n",
    "    # Combine local exploitation (cluster) + global exploration\n",
    "    best_idx = np.argmax(preds)\n",
    "    best_query = np.clip(\n",
    "        0.7 * centroid + 0.3 * candidates[best_idx], 0.0, 1.0\n",
    "    )\n",
    "    query_str = \"-\".join([f\"{x:.6f}\" for x in best_query])\n",
    "\n",
    "    print(f\"  Best predicted output: {preds[best_idx]:.4f}\")\n",
    "    print(f\"  Cluster-based centroid output: {cluster_pred:.4f} (silhouette={sil_score:.3f})\")\n",
    "    print(f\"  Query to submit: {query_str}\")\n",
    "\n",
    "    # --- PLOT & LOG ---\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(y_combined, model.predict(X_combined), c=labels, cmap=\"tab10\", edgecolor='k')\n",
    "    plt.plot([y_combined.min(), y_combined.max()],\n",
    "             [y_combined.min(), y_combined.max()], 'r--', lw=2)\n",
    "    plt.title(f\"Function {i} - Actual vs Predicted (Week 11)\")\n",
    "    plt.xlabel(\"Actual y\"); plt.ylabel(\"Predicted y\")\n",
    "    plt.savefig(os.path.join(log_folder, f\"function_{i}_fit.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    cluster_info[f\"Function_{i}\"] = {\n",
    "        \"data_points\": len(X_combined),\n",
    "        \"clusters\": k_opt,\n",
    "        \"silhouette\": sil_score,\n",
    "        \"cluster_means\": cluster_means,\n",
    "        \"best_cluster\": int(best_cluster),\n",
    "        \"centroid\": centroid.tolist(),\n",
    "        \"centroid_predicted_output\": float(cluster_pred),\n",
    "        \"selected_query\": best_query.tolist()\n",
    "    }\n",
    "\n",
    "    queries_out.append(f\"Function {i}: {query_str}\")\n",
    "\n",
    "# =========================\n",
    "# SAVE RESULTS\n",
    "# =========================\n",
    "with open(\"week11_queries.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(queries_out))\n",
    "\n",
    "with open(os.path.join(log_folder, \"week11_cluster_info.json\"), \"w\") as f:\n",
    "    json.dump(cluster_info, f, indent=4)\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved all week11 queries to week11_queries.txt\")\n",
    "print(\"ðŸ“Š Cluster analysis logs saved under week11_logs/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml311)",
   "language": "python",
   "name": "ml311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
